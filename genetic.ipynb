{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#base file path\n","path = '/content/drive/Shareddrives/soft computing project test'"],"metadata":{"id":"N23EhX3v6w8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports and helper functions"],"metadata":{"id":"aD4MP6Mb6yXj"}},{"cell_type":"code","source":["import os                                 # For file and directory operations\n","import cv2                                # For image processing (reading and resizing images)\n","import numpy as np                        # For numerical operations (e.g., reshaping arrays)\n","import pandas as pd                       # For data manipulation (reading CSV, manipulating data)\n","from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n","from sklearn.utils import shuffle          # For shuffling data\n","from tensorflow.keras import layers, models  # For creating and managing deep learning models\n","from tensorflow.keras.models import Model  # For managing models\n","import tensorflow as tf                   # For deep learning operations\n","from imblearn.over_sampling import SMOTE  # For oversampling imbalanced datasets\n","from google.colab import drive            # For mounting Google Drive\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","driveLocation = path\n","%cd $driveLocation"],"metadata":{"id":"_VFYvgD-_BGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function to load images from a directory\n","def load_images_from_folder(folder_path):\n","    dataset = np.load(folder_path)\n","    return dataset['images'], dataset['image_files']"],"metadata":{"id":"r7O339Nk_R0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function to load the clusters\n","def get_clusters(image_files, input_file='cluster_assignments.csv'):\n","    df = pd.read_csv(input_file)\n","    # map the clusters back to the provided image files\n","    cluster_dict = dict(zip(df['Image File'], df['Cluster']))\n","\n","    # ensure clusters are returned in the same order as image_files\n","    clusters = [cluster_dict[img] for img in image_files]\n","\n","    return clusters"],"metadata":{"id":"QtcOACxV_VF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# load the dataset"],"metadata":{"id":"vfBLk3aG64pG"}},{"cell_type":"code","source":["#load images from the unlabeled dataset\n","unlabeled_dataset_folder = os.path.join(path, 'dataset.npz')\n","images, image_files = load_images_from_folder(unlabeled_dataset_folder)\n","# Let's assume cluster 0 corresponds to 'no ship' and cluster 1 corresponds to 'ship'\n","\n","#get the clusters\n","clusters = get_clusters(image_files, 'cluster_assignments_kmeans.csv')\n","\n","# Convert labels to a binary format (0 or 1)\n","labels = np.array(clusters)  # Cluster 0 as 'no ship', Cluster 1 as 'ship'\n","\n","\n","#count how many images are in each class\n","unique_labels, counts = np.unique(labels, return_counts=True)\n","\n","# Print the results\n","for label, count in zip(unique_labels, counts):\n","    print(f\"Label {label}: {count} images\")\n","\n","\n","# Split the data into training and a temporary set for validation and testing\n","X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n","\n","# Create SMOTE object\n","smote = SMOTE(random_state=42)\n","\n","# Reshape the image data if necessary\n","X_train_flattened = X_train.reshape(X_train.shape[0], -1)  # Reshape to (num_samples, 80*80*3)\n","\n","# Apply SMOTE to oversample the minority class\n","X_resampled, y_resampled = smote.fit_resample(X_train_flattened, y_train)\n","\n","#Reshape the oversampled images back to their original dimensions\n","X_train_balanced = X_resampled.reshape(X_resampled.shape[0], 80, 80, 3)  # Reshape back to (num_samples, 80, 80, 3)\n","\n","# Shuffle the balanced training set\n","X_train_balanced, y_train_balanced = shuffle(X_train_balanced, y_resampled, random_state=42)\n","\n","#  Now split the temporary set into validation and test sets (70% val, 30% test of the temp set)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n","\n","print(f\"Number of images in the training set: {len(X_train_balanced)}\")\n","print(f\"Number of images in the validation set: {len(X_val)}\")\n","print(f\"Number of images in the testing set: {len(X_test)}\")\n","\n","# Create TensorFlow Dataset objects from the NumPy arrays\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train_balanced, y_train_balanced))\n","val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","\n","# Normalize the images (scaling between 0 and 1)\n","def normalize(image, label):\n","    image = tf.cast(image, tf.float32) / 255.0\n","    return image, label\n","\n","# Apply normalization lazily on each batch\n","train_dataset = train_dataset.map(normalize)\n","val_dataset = val_dataset.map(normalize)\n","test_dataset = test_dataset.map(normalize)\n","\n","# Set the batch size and shuffle the training dataset\n","batch_size = 32\n","train_dataset = train_dataset.batch(batch_size).shuffle(buffer_size=1000)\n","val_dataset = val_dataset.batch(batch_size)\n","test_dataset = test_dataset.batch(batch_size)"],"metadata":{"id":"sJh1y0FnY4h4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evolve the genetic algorithm to find the optimal model architecture"],"metadata":{"id":"JhThKYZz66bw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1YDFdVa8VFQ"},"outputs":[],"source":["# Genetic Algorithm Configuration\n","POPULATION_SIZE = 10\n","GENERATIONS = 5\n","MUTATION_RATE = 0.1\n","\n","# Define possible configurations\n","activation_type_conv_layers = ['relu', 'sigmoid', 'tanh']\n","activation_type_dense_layers = ['relu', 'sigmoid', 'tanh']\n","pooling_types = [None, 'max', 'average'] # pooling type for each convolution block\n","num_conv_layers_block1 = range(1, 4)  # 1 to 3 convolution layers per block\n","batch_norm_block1 = [True, False]\n","num_conv_layers_block2 = range(1, 4)\n","batch_norm_block2 = [True, False]\n","num_conv_layers_block3 = range(1, 4)\n","batch_norm_block3 = [True, False]\n","num_dense_layers_options = range(1, 4)  # 1 to 3 dense layers\n","num_of_nodes = [16, 32, 64]  # Nodes per dense layer\n","dropout_options = [0.0, 0.3, 0.5]  # Dropout rate options\n","\n","class Individual:\n","    def __init__(self):\n","        # Randomly choose configuration per block\n","        self.num_conv_layers_block1 = np.random.choice(num_conv_layers_block1)\n","        self.batch_norm_block1 = np.random.choice(batch_norm_block1)\n","        self.num_conv_layers_block2 = np.random.choice(num_conv_layers_block2)\n","        self.batch_norm_block2 = np.random.choice(batch_norm_block2)\n","        self.num_conv_layers_block3 = np.random.choice(num_conv_layers_block3)\n","        self.batch_norm_block3 = np.random.choice(batch_norm_block3)\n","\n","        # Randomly choose activation for conv and dense layers\n","        self.conv_activation = np.random.choice(activation_type_conv_layers)\n","        self.dense_activation = np.random.choice(activation_type_dense_layers)\n","\n","        # Pooling type for each convolutional block\n","        self.pooling_layers = [np.random.choice(pooling_types) for _ in range(3)]\n","\n","        # Dense layer configurations\n","        self.num_dense_layers = np.random.choice(num_dense_layers_options)\n","        self.num_nodes_per_layer = np.random.choice(num_of_nodes, self.num_dense_layers)\n","\n","        # Dropout\n","        self.dropout_rate = np.random.choice(dropout_options)\n","\n","        # Model and fitness attributes\n","        self.model = None\n","        self.fitness = 0\n","\n","    def add_conv_block(self, model, num_layers, use_batch_norm, pooling_type):\n","        for _ in range(num_layers):\n","            model.add(layers.Conv2D(32, (3, 3), activation=self.conv_activation, padding='same'))\n","            if use_batch_norm:\n","                model.add(layers.BatchNormalization())\n","        if pooling_type == 'max':\n","            model.add(layers.MaxPooling2D((2, 2)))\n","        elif pooling_type == 'average':\n","            model.add(layers.AveragePooling2D((2, 2)))\n","\n","    def build_model(self):\n","        model = models.Sequential()\n","        model.add(layers.InputLayer(input_shape=(80, 80, 3)))\n","\n","        # Add convolution blocks with respective configurations\n","        self.add_conv_block(model, self.num_conv_layers_block1, self.batch_norm_block1, self.pooling_layers[0])\n","        self.add_conv_block(model, self.num_conv_layers_block2, self.batch_norm_block2, self.pooling_layers[1])\n","        self.add_conv_block(model, self.num_conv_layers_block3, self.batch_norm_block3, self.pooling_layers[2])\n","\n","        model.add(layers.Flatten())\n","\n","        # Add dense layers with respective configurations\n","        for nodes in self.num_nodes_per_layer:\n","            model.add(layers.Dense(nodes, activation=self.dense_activation))\n","            if self.dropout_rate > 0:\n","                model.add(layers.Dropout(self.dropout_rate))\n","\n","        model.add(layers.Dense(2, activation='softmax'))  # Output layer for binary classification\n","        self.model = model\n","        return model\n","\n","    def evaluate(self):\n","        self.model = self.build_model()\n","        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        self.model.fit(train_dataset, epochs=10, validation_data=val_dataset, verbose=0)\n","        self.fitness = self.model.evaluate(test_dataset, verbose=0)[1]  # Get accuracy\n","        print(f\"Evaluated Individual - Fitness: {self.fitness:.4f}\")\n","\n","def mutate(individual):\n","    # Randomly mutate an individual's properties for convolutional blocks\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.num_conv_layers_block1 = np.random.choice(num_conv_layers_block1)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.num_conv_layers_block2 = np.random.choice(num_conv_layers_block2)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.num_conv_layers_block3 = np.random.choice(num_conv_layers_block3)\n","\n","    # Mutate batch normalization settings\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.batch_norm_block1 = np.random.choice(batch_norm_block1)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.batch_norm_block2 = np.random.choice(batch_norm_block2)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.batch_norm_block3 = np.random.choice(batch_norm_block3)\n","\n","    # Mutate activation functions\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.conv_activation = np.random.choice(activation_type_conv_layers)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.dense_activation = np.random.choice(activation_type_dense_layers)\n","\n","    # Mutate pooling types for each block\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.pooling_layers = [np.random.choice(pooling_types) for _ in range(3)]\n","\n","    # Mutate dense layer properties\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.num_dense_layers = np.random.choice(num_dense_layers_options)\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.num_nodes_per_layer = np.random.choice(num_of_nodes, individual.num_dense_layers)\n","\n","    # Mutate dropout rate\n","    if np.random.rand() < MUTATION_RATE:\n","        individual.dropout_rate = np.random.choice(dropout_options)\n","\n","def crossover(parent1, parent2):\n","    # Create a new individual by combining the properties of two parents\n","    child = Individual()\n","\n","    # Crossover for convolutional block layers\n","    child.num_conv_layers_block1 = np.random.choice([parent1.num_conv_layers_block1, parent2.num_conv_layers_block1])\n","    child.num_conv_layers_block2 = np.random.choice([parent1.num_conv_layers_block2, parent2.num_conv_layers_block2])\n","    child.num_conv_layers_block3 = np.random.choice([parent1.num_conv_layers_block3, parent2.num_conv_layers_block3])\n","\n","    # Crossover for batch normalization settings\n","    child.batch_norm_block1 = np.random.choice([parent1.batch_norm_block1, parent2.batch_norm_block1])\n","    child.batch_norm_block2 = np.random.choice([parent1.batch_norm_block2, parent2.batch_norm_block2])\n","    child.batch_norm_block3 = np.random.choice([parent1.batch_norm_block3, parent2.batch_norm_block3])\n","\n","    # Crossover for activation functions\n","    child.conv_activation = parent1.conv_activation if np.random.rand() > 0.5 else parent2.conv_activation\n","    child.dense_activation = parent1.dense_activation if np.random.rand() > 0.5 else parent2.dense_activation\n","\n","    # Crossover for pooling layers and dense layers\n","    child.pooling_layers = parent1.pooling_layers if np.random.rand() > 0.5 else parent2.pooling_layers\n","    child.num_dense_layers = np.random.choice([parent1.num_dense_layers, parent2.num_dense_layers])\n","    child.num_nodes_per_layer = parent1.num_nodes_per_layer if np.random.rand() > 0.5 else parent2.num_nodes_per_layer\n","\n","    # Crossover for dropout rate\n","    child.dropout_rate = parent1.dropout_rate if np.random.rand() > 0.5 else parent2.dropout_rate\n","\n","    return child\n","\n","# Initialize the population\n","population = [Individual() for _ in range(POPULATION_SIZE)]\n","\n","# Genetic Algorithm Process\n","for generation in range(GENERATIONS):\n","    # Evaluate fitness of each individual\n","    for individual in population:\n","        individual.evaluate()\n","\n","    # Sort population by fitness\n","    population.sort(key=lambda ind: ind.fitness, reverse=True)\n","\n","    # Print best architecture and its fitness\n","    best_individual = population[0]\n","    print(f\"Generation {generation}, Best fitness: {best_individual.fitness:.4f}, Architecture: {vars(best_individual)}\")\n","\n","    # Create new population\n","    new_population = population[:2]  # Keep the top 2\n","    while len(new_population) < POPULATION_SIZE:\n","        parent1, parent2 = np.random.choice(population[:5], 2)  # Select parents from top individuals\n","        child = crossover(parent1, parent2)\n","        mutate(child)\n","        new_population.append(child)\n","\n","    population = new_population  # Move to the next generation\n","\n","print(\"Optimization Complete\")\n"]}]}